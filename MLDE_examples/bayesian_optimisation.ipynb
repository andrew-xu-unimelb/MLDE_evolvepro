{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turbo import TurboM\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average % of high activity mutants:150.0%\n"
     ]
    }
   ],
   "source": [
    "# loading UMAP embeddings & labels\n",
    "umap_df = pd.read_csv(\"./data/umap_fit.csv\")\n",
    "umap_df = umap_df.iloc[:, 1:3] # select only first 2 dimensions, can change for up to d dimensions\n",
    "labels_df = pd.read_csv(\"./data/labels_sorted.csv\")\n",
    "labels = labels_df['activity'] # I prefer 'activity' over 'activity_scaled' because the latter is unobtainable during actual experimentation\n",
    "\n",
    "\n",
    "# running through 10 iterations of local bayesian optimisation to get an average of results\n",
    "for i in range(0, 10):\n",
    "\n",
    "    # defining the objective function \n",
    "    class DataObjective:\n",
    "        def __init__(self, data, labels):\n",
    "            \"\"\"\n",
    "            Initializes the objective function with data and labels.\n",
    "            Keeps track of which data points have already been used.\n",
    "            \"\"\"\n",
    "            self.data = np.asarray(data)\n",
    "            self.labels = np.asarray(labels)\n",
    "            self.used = np.zeros(len(self.data), dtype=bool)\n",
    "        \n",
    "            # define lower and upper bounds of embeddings for optimisation\n",
    "            self.lb = self.data.min(axis=0)\n",
    "            self.ub = self.data.max(axis=0)\n",
    "        \n",
    "        def __call__(self, x):\n",
    "            \"\"\"\n",
    "            Given a candidate point x, returns the label (activity) of the closest row in the dataset.\n",
    "            Ensures that already used rows are not considered.\n",
    "            \"\"\"\n",
    "            x = np.asarray(x).reshape(-1)\n",
    "            if x.shape[0] != self.data.shape[1]:\n",
    "                raise ValueError(\"Input dimension does not match the data dimension.\")\n",
    "            \n",
    "            # calculate euclidean distances \n",
    "            #TODO: can potentially change distance metric to something else\n",
    "            distances = np.linalg.norm(self.data - x, axis=1)\n",
    "\n",
    "            # ignore rows that have already been used\n",
    "            distances[self.used] = np.inf\n",
    "\n",
    "            # if all points have been used, raise error\n",
    "            if np.all(np.isinf(distances)):\n",
    "                raise ValueError(\"All data used.\")\n",
    "\n",
    "            # find idnex of row with the minimum distance\n",
    "            nearest_idx = np.argmin(distances)\n",
    "\n",
    "            # mark this row as used\n",
    "            self.used[nearest_idx] = True\n",
    "\n",
    "            # retrieve the label (activity) of the selected row\n",
    "            output = self.labels[nearest_idx]\n",
    "            return -output # returning the negative value since TuRBO by default optimises for global minima\n",
    "\n",
    "\n",
    "    f = DataObjective(umap_df, labels)\n",
    "\n",
    "    # running bayesian optimisation\n",
    "    outcome_list = [] # list to store the percentage of high-activity mutants\n",
    "\n",
    "    turbo_m = TurboM(\n",
    "        f=f,  # objective function\n",
    "        lb=f.lb,  # Numpy array specifying lower bounds\n",
    "        ub=f.ub,  # Numpy array specifying upper bounds\n",
    "        n_init=16,  # Number of initial bounds from an Symmetric Latin hypercube design\n",
    "        max_evals=160,  # Maximum number of evaluations\n",
    "        n_trust_regions=2,  # Number of trust regions\n",
    "        batch_size=8,  # How large batch size TuRBO uses\n",
    "        verbose=False,  # Print information from each batch\n",
    "        use_ard=True,  # Use ARD (Automatic Relevance Determination) for Gaussian Process kernel\n",
    "        max_cholesky_size=2000,  # When we switch from Cholesky to Lanczos decomposition\n",
    "        n_training_steps=30,  # Number of steps of ADAM to learn the hypers\n",
    "        min_cuda=1024,  # Run on the CPU for small datasets\n",
    "        device=\"cpu\",  # \"cpu\" or \"cuda\"\n",
    "        dtype=\"float64\",  # float64 or float32\n",
    "        )\n",
    "    \n",
    "    turbo_m.optimize() # start optimisation process\n",
    "\n",
    "    # retrieve the optimised data points and their corresponding function values\n",
    "    X = turbo_m.X\n",
    "    fX = turbo_m.fX\n",
    "    selected_protein_df = pd.DataFrame(X)\n",
    "    selected_protein_df['activity'] = -fX\n",
    "\n",
    "    # calculating % of high activity mutants\n",
    "    high_activity_proteins = selected_protein_df[selected_protein_df['activity']>=2.8] # for Jones data, anything with activity >2.8 is considered \"high activity\" (>50th percentile of activity)\n",
    "    outcome_list.append(len(high_activity_proteins)/16)\n",
    "\n",
    "print(f\"average % of high activity mutants:{sum(outcome_list)/len(outcome_list)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
